{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anuragrajput/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp \n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"google.protobuf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1755493620.217742 1332206 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules from the Mediapipe library for hand tracking\n",
    "mp_hands = mp.solutions.hands  # Mediapipe's hand tracking solution\n",
    "mp_drawing = mp.solutions.drawing_utils  # Utility for drawing landmarks and connections on images\n",
    "mp_drawing_styles = mp.solutions.drawing_styles  # Provides pre-defined drawing styles for landmarks\n",
    "\n",
    "# Initialize the Hands model from Mediapipe\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,  # Set to False for video streams; detection happens only in the first frame\n",
    "    max_num_hands=1,  # Track at most one hand\n",
    "    min_detection_confidence=0.9  # Minimum confidence score for hand detection\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0:\"0\", 1:\"1\", 2:\"2\", 3:\"3\", 4:\"4\", 5:\"5\", 6:\"6\", 7:\"7\", 8:\"8\", 9:\"9\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1755493620.232842 1332318 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1755493620.237429 1332319 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/Users/anuragrajput/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.4.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/anuragrajput/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.4.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open(\"./rf_model.p\", \"rb\") as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = model[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1755493622.664978 1332322 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m x_coordinates, y_coordinates \u001b[38;5;241m=\u001b[39m [], []  \u001b[38;5;66;03m# To store the x and y coordinates of landmarks\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Capture another frame (redundant call, you might only need one)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Get the dimensions of the frame (height, width, and color channels)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m height, width, _ \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Start capturing video from the default camera (index 0)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Start an infinite loop to process each video frame in real-time\n",
    "while True:\n",
    "    \n",
    "    # Read the next frame from the video capture\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Lists to store normalized landmark coordinates and x/y coordinates\n",
    "    normalized_landmarks = []  # To store normalized coordinates\n",
    "    x_coordinates, y_coordinates = [], []  # To store the x and y coordinates of landmarks\n",
    "\n",
    "    # Capture another frame (redundant call, you might only need one)\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Get the dimensions of the frame (height, width, and color channels)\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Convert the frame from BGR (used by OpenCV) to RGB (used by Mediapipe)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame with the Mediapipe Hands model to detect hands\n",
    "    processed_image = hands.process(frame_rgb)\n",
    "    \n",
    "    # Get hand landmarks (if any are detected) from the processed image\n",
    "    hand_landmarks = processed_image.multi_hand_landmarks\n",
    "\n",
    "    # If hand landmarks are detected in the frame\n",
    "    if hand_landmarks:\n",
    "        # Loop through the detected hand landmarks\n",
    "        for hand_landmark in hand_landmarks:\n",
    "            # Draw the hand landmarks and connections on the frame using the predefined styles\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,  # The original frame\n",
    "                hand_landmark,  # Detected landmarks for the hand\n",
    "                mp_hands.HAND_CONNECTIONS,  # Hand connections to be drawn\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),  # Style for hand landmarks\n",
    "                mp_drawing_styles.get_default_hand_connections_style()  # Style for hand connections\n",
    "            )\n",
    "\n",
    "            # Loop through the landmarks of the hand and extract coordinates\n",
    "            for hand_landmark in hand_landmarks:\n",
    "                landmark_coordinates = hand_landmark.landmark\n",
    "\n",
    "                # Store x and y coordinates of the landmarks\n",
    "                for coordinates in landmark_coordinates:\n",
    "                    x_coordinates.append(coordinates.x)  # Append x coordinates (normalized 0-1)\n",
    "                    y_coordinates.append(coordinates.y)  # Append y coordinates (normalized 0-1)\n",
    "\n",
    "                # Find the minimum x and y values (to be used for normalization)\n",
    "                min_x, min_y = min(x_coordinates), min(y_coordinates)\n",
    "\n",
    "                # Normalize the x and y coordinates based on the minimum values\n",
    "                for coordinates in landmark_coordinates:\n",
    "                    normalized_x = coordinates.x - min_x  # Normalize x\n",
    "                    normalized_y = coordinates.y - min_y  # Normalize y\n",
    "                    normalized_landmarks.extend((normalized_x, normalized_y))  # Store normalized values\n",
    "\n",
    "        # Convert normalized coordinates to pixel values for bounding box display\n",
    "        x1 = int(min(x_coordinates) * width)  # Minimum x coordinate scaled to the frame width\n",
    "        y1 = int(min(y_coordinates) * height)  # Minimum y coordinate scaled to the frame height\n",
    "        x2 = int(max(x_coordinates) * width)  # Maximum x coordinate scaled to the frame width\n",
    "        y2 = int(max(y_coordinates) * height)  # Maximum y coordinate scaled to the frame height\n",
    "\n",
    "        # Prepare the normalized landmarks to be used for model prediction\n",
    "        sample = np.asarray(normalized_landmarks).reshape(1, -1)  # Reshape the landmarks into a sample\n",
    "        pred = rf_model.predict(sample)  # Use a pre-trained random forest model to make predictions\n",
    "\n",
    "        # Get the predicted character/label (from a pre-defined labels list) based on model output\n",
    "        predicted_character = labels[int(pred[0])]\n",
    "\n",
    "        # Draw a rectangle around the detected hand based on the bounding box\n",
    "        cv2.rectangle(frame, (x1 + 10, y1 + 10), (x2, y2), (100, 200, 100), 4)  # Green rectangle\n",
    "\n",
    "        # Display the predicted character as text on the frame\n",
    "        cv2.putText(img=frame,                          # Image/frame on which to put text\n",
    "                    text=predicted_character,           # Text to display (predicted character)\n",
    "                    org=(x1, y1),                       # Text position (top-left corner of the bounding box)\n",
    "                    fontFace=cv2.FONT_HERSHEY_SIMPLEX,  # Font type\n",
    "                    fontScale=2,                        # Font scale (size)\n",
    "                    color=(0, 0, 0),                    # Text color (black)\n",
    "                    thickness=3,                        # Thickness of the text\n",
    "                    lineType=cv2.LINE_AA)               # Anti-aliased line for smooth text rendering\n",
    "\n",
    "    # Display the video frame with landmarks, bounding box, and predicted character in a window\n",
    "    cv2.imshow(\"Video Mode\", frame)\n",
    "\n",
    "    # Exit the loop if the \"q\" key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the video capture when the loop ends and destroy all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
