{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp \n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"google.protobuf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules from the Mediapipe library for hand tracking\n",
    "mp_hands = mp.solutions.hands  # Mediapipe's hand tracking solution\n",
    "mp_drawing = mp.solutions.drawing_utils  # Utility for drawing landmarks and connections on images\n",
    "mp_drawing_styles = mp.solutions.drawing_styles  # Provides pre-defined drawing styles for landmarks\n",
    "\n",
    "# Initialize the Hands model from Mediapipe\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,  # Set to False for video streams; detection happens only in the first frame\n",
    "    max_num_hands=1,  # Track at most one hand\n",
    "    min_detection_confidence=0.9  # Minimum confidence score for hand detection\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0:\"0\", 1:\"1\", 2:\"2\", 3:\"3\", 4:\"4\", 5:\"5\", 6:\"6\", 7:\"7\", 8:\"8\", 9:\"9\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open(\"./rf_model.p\", \"rb\") as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = model[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start capturing video from the default camera (index 0)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Start an infinite loop to process each video frame in real-time\n",
    "while True:\n",
    "    \n",
    "    # Read the next frame from the video capture\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Lists to store normalized landmark coordinates and x/y coordinates\n",
    "    normalized_landmarks = []  # To store normalized coordinates\n",
    "    x_coordinates, y_coordinates = [], []  # To store the x and y coordinates of landmarks\n",
    "\n",
    "    # Capture another frame (redundant call, you might only need one)\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Get the dimensions of the frame (height, width, and color channels)\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Convert the frame from BGR (used by OpenCV) to RGB (used by Mediapipe)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame with the Mediapipe Hands model to detect hands\n",
    "    processed_image = hands.process(frame_rgb)\n",
    "    \n",
    "    # Get hand landmarks (if any are detected) from the processed image\n",
    "    hand_landmarks = processed_image.multi_hand_landmarks\n",
    "\n",
    "    # If hand landmarks are detected in the frame\n",
    "    if hand_landmarks:\n",
    "        # Loop through the detected hand landmarks\n",
    "        for hand_landmark in hand_landmarks:\n",
    "            # Draw the hand landmarks and connections on the frame using the predefined styles\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,  # The original frame\n",
    "                hand_landmark,  # Detected landmarks for the hand\n",
    "                mp_hands.HAND_CONNECTIONS,  # Hand connections to be drawn\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),  # Style for hand landmarks\n",
    "                mp_drawing_styles.get_default_hand_connections_style()  # Style for hand connections\n",
    "            )\n",
    "\n",
    "            # Loop through the landmarks of the hand and extract coordinates\n",
    "            for hand_landmark in hand_landmarks:\n",
    "                landmark_coordinates = hand_landmark.landmark\n",
    "\n",
    "                # Store x and y coordinates of the landmarks\n",
    "                for coordinates in landmark_coordinates:\n",
    "                    x_coordinates.append(coordinates.x)  # Append x coordinates (normalized 0-1)\n",
    "                    y_coordinates.append(coordinates.y)  # Append y coordinates (normalized 0-1)\n",
    "\n",
    "                # Find the minimum x and y values (to be used for normalization)\n",
    "                min_x, min_y = min(x_coordinates), min(y_coordinates)\n",
    "\n",
    "                # Normalize the x and y coordinates based on the minimum values\n",
    "                for coordinates in landmark_coordinates:\n",
    "                    normalized_x = coordinates.x - min_x  # Normalize x\n",
    "                    normalized_y = coordinates.y - min_y  # Normalize y\n",
    "                    normalized_landmarks.extend((normalized_x, normalized_y))  # Store normalized values\n",
    "\n",
    "        # Convert normalized coordinates to pixel values for bounding box display\n",
    "        x1 = int(min(x_coordinates) * width)  # Minimum x coordinate scaled to the frame width\n",
    "        y1 = int(min(y_coordinates) * height)  # Minimum y coordinate scaled to the frame height\n",
    "        x2 = int(max(x_coordinates) * width)  # Maximum x coordinate scaled to the frame width\n",
    "        y2 = int(max(y_coordinates) * height)  # Maximum y coordinate scaled to the frame height\n",
    "\n",
    "        # Prepare the normalized landmarks to be used for model prediction\n",
    "        sample = np.asarray(normalized_landmarks).reshape(1, -1)  # Reshape the landmarks into a sample\n",
    "        pred = rf_model.predict(sample)  # Use a pre-trained random forest model to make predictions\n",
    "\n",
    "        # Get the predicted character/label (from a pre-defined labels list) based on model output\n",
    "        predicted_character = labels[int(pred[0])]\n",
    "\n",
    "        # Draw a rectangle around the detected hand based on the bounding box\n",
    "        cv2.rectangle(frame, (x1 + 10, y1 + 10), (x2, y2), (100, 200, 100), 4)  # Green rectangle\n",
    "\n",
    "        # Display the predicted character as text on the frame\n",
    "        cv2.putText(img=frame,                          # Image/frame on which to put text\n",
    "                    text=predicted_character,           # Text to display (predicted character)\n",
    "                    org=(x1, y1),                       # Text position (top-left corner of the bounding box)\n",
    "                    fontFace=cv2.FONT_HERSHEY_SIMPLEX,  # Font type\n",
    "                    fontScale=2,                        # Font scale (size)\n",
    "                    color=(0, 0, 0),                    # Text color (black)\n",
    "                    thickness=3,                        # Thickness of the text\n",
    "                    lineType=cv2.LINE_AA)               # Anti-aliased line for smooth text rendering\n",
    "\n",
    "    # Display the video frame with landmarks, bounding box, and predicted character in a window\n",
    "    cv2.imshow(\"Video Mode\", frame)\n",
    "\n",
    "    # Exit the loop if the \"q\" key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the video capture when the loop ends and destroy all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
